import streamlit as st
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.tools import tool
from datetime import datetime
import pytz
from dotenv import load_dotenv
import os

# --- [ìˆ˜ì •] Tavily ê²€ìƒ‰ ë„êµ¬ import ---
from langchain_community.tools.tavily_search import TavilySearchResults

# --- .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ ---
load_dotenv()
# ------------------------------------


# --- .env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ ---
google_api_key = os.getenv("GOOGLE_API_KEY")
# --- [ì¶”ê°€] Tavily API í‚¤ ë¡œë“œ ---
tavily_api_key = os.getenv("TAVILY_API_KEY")

if not google_api_key:
    st.info("Google API í‚¤(.env ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (GOOGLE_API_KEY)")
    st.stop()

# --- [ì¶”ê°€] Tavily í‚¤ í™•ì¸ ---
if not tavily_api_key:
    st.info("Tavily API í‚¤(.env ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (TAVILY_API_KEY)")
    st.stop()
# ------------------------------------------

# --- ëª¨ë¸ ì´ˆê¸°í™”: gemini-2.5-flash (ë³€ê²½ ì—†ìŒ) ---
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    google_api_key=google_api_key,
    api_version="v1"
)
# ---------------------------------

# --- ë„êµ¬ í•¨ìˆ˜ ì •ì˜ ---

@tool
def get_current_time(timezone: str, location: str) -> str:
    """í˜„ì¬ ì‹œê°ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜."""
    try:
        tz = pytz.timezone(timezone)
        now = datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")
        result = f'{timezone} ({location}) í˜„ì¬ì‹œê° {now}'
        print(result)
        return result
    except pytz.UnknownTimeZoneError:
        return f"ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì„ì¡´: {timezone}"
    
# --- [ìˆ˜ì •] get_web_search í•¨ìˆ˜ë¥¼ Tavily ë²„ì „ìœ¼ë¡œ êµì²´ ---
@tool
def get_web_search(query: str) -> str:
    """
    Tavilyë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜.
    'search_period' ì¸ìëŠ” ë°›ì§€ ì•ŠìŠµë‹ˆë‹¤.
    
    Args:
        query (str): ê²€ìƒ‰ì–´

    Returns:
        str: ê²€ìƒ‰ ê²°ê³¼ (Snippet, Title, URL í˜•ì‹)
    """
    print('-------- TAVILY WEB SEARCH --------')
    print(query)

    # Tavily ê²€ìƒ‰ ë„êµ¬ ì´ˆê¸°í™” (k=5: 5ê°œì˜ ê²°ê³¼ ìš”ì²­)
    search = TavilySearchResults(
        k=5, 
        tavily_api_key=tavily_api_key # API í‚¤ ëª…ì‹œì  ì „ë‹¬
    ) 
    
    try:
        # TavilyëŠ” [ {'url': ..., 'content': ...}, ... ] í˜•ì‹ì˜ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜
        docs = search.invoke(query)
        
        results_str_list = []
        for doc in docs:
            snippet = doc.get('content', 'ë‚´ìš© ì—†ìŒ')
            title = doc.get('title', 'ì œëª© ì—†ìŒ')
            url = doc.get('url', 'ì¶œì²˜ ì—†ìŒ')
            results_str_list.append(f"Snippet: {snippet}\nTitle: {title}\nURL: {url}")
            
        return "\n\n;\n\n".join(results_str_list)

    except Exception as e:
        print(f"Tavily ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
# --- DuckDuckGo ë²„ì „ì˜ get_web_searchëŠ” ì™„ì „íˆ ì‚­ì œ ---


# --- [ìˆ˜ì •] ë„êµ¬ ë°”ì¸ë”© (í•œ ë²ˆë§Œ ì •ì˜) ---
tools = [get_current_time, get_web_search]
tool_dict = {
    "get_current_time": get_current_time, 
    "get_web_search": get_web_search
}

llm_with_tools = llm.bind_tools(tools)
# ---------------------------------


# ì‚¬ìš©ìì˜ ë©”ì‹œì§€ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ í•¨ìˆ˜ (ì›ë³¸ê³¼ ë™ì¼, ë³€ê²½ ì—†ìŒ)
def get_ai_response(messages):
    response_stream = llm_with_tools.stream(messages)
    
    full_response = None
    final_text_content = "" # ìµœì¢… í…ìŠ¤íŠ¸ ë‹µë³€ì„ ëˆ„ì 

    for chunk in response_stream:
        if chunk.content:
            yield chunk.content
            final_text_content += chunk.content
        
        if full_response is None:
            full_response = chunk
        else:
            full_response += chunk

    if full_response and full_response.tool_calls:
        st.session_state.messages.append(full_response)
        
        tool_outputs = []
        
        for tool_call in full_response.tool_calls:
            selected_tool = tool_dict[tool_call['name']]
            
            try:
                # ëª¨ë¸ì´ Tavily ë„êµ¬ ëª…ì„¸ì— ë”°ë¼ {'query': '...'}ë§Œ ì „ë‹¬í•  ê²ƒì„
                tool_output = selected_tool.invoke(tool_call['args'])
            except Exception as e:
                tool_output = f"Tool Error: {e}"
            
            print(f"Tool: {tool_call['name']}, Args: {tool_call['args']}, Output: {tool_output}")

            tool_outputs.append(
                ToolMessage(
                    content=str(tool_output),
                    tool_call_id=tool_call['id']
                )
            )

        for msg in tool_outputs:
            st.chat_message("tool").write(msg.content)
        st.session_state.messages.extend(tool_outputs)
        
        for chunk_content in get_ai_response(st.session_state.messages):
            yield chunk_content
            
    elif final_text_content:
        pass


# Streamlit ì•±
st.title("ğŸ’¬ Google Gemini + Tavily Search") # (ì œëª© ìˆ˜ì •)

# ìŠ¤íŠ¸ë¦¼ë¦¿ session_stateì— ë©”ì‹œì§€ ì €ì¥ (ë³€ê²½ ì—†ìŒ)
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        SystemMessage("ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë•ê¸° ìœ„í•´ ìµœì„ ì„ ë‹¤í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ë´‡ì´ë‹¤. "), 
        AIMessage("ë¬´ì—‡ì„ ì•Œê³  ì‹¶ë‹ˆ?")
    ]

# ìŠ¤íŠ¸ë¦¼ë¦¿ í™”ë©´ì— ë©”ì‹œì§€ ì¶œë ¥ (ì›ë³¸ê³¼ ë™ì¼, ë³€ê²½ ì—†ìŒ)
for msg in st.session_state.messages:
    if msg.content:
        if isinstance(msg, SystemMessage):
            st.chat_message("system").write(msg.content)
        elif isinstance(msg, AIMessage):
            if not msg.tool_calls:
                st.chat_message("assistant").write(msg.content)
        elif isinstance(msg, HumanMessage):
            st.chat_message("user").write(msg.content)
        elif isinstance(msg, ToolMessage):
            st.chat_message("tool").write(f"Tool Output:\n```\n{msg.content}\n```")

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ (ì›ë³¸ê³¼ ë™ì¼, ë³€ê²½ ì—†ìŒ)
if prompt := st.chat_input():
    st.chat_message("user").write(prompt)
    st.session_state.messages.append(HumanMessage(prompt))

    response_generator = get_ai_response(st.session_state["messages"])
    result = st.chat_message("assistant").write_stream(response_generator)
    
    if result:
        st.session_state["messages"].append(AIMessage(result))