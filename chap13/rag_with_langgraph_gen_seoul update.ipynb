{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465e9fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# í™˜ê²½ ê°ì§€ ë° API í‚¤ ë¡œë“œ\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
    "    print(\"âœ… êµ¬ê¸€ ì½”ë© í™˜ê²½: API í‚¤ ë¡œë“œ ì™„ë£Œ\")\n",
    "except ImportError:\n",
    "    # VS Code / Codespaces í™˜ê²½ (.env íŒŒì¼ ì‚¬ìš©)\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "        print(\"âš ï¸ ê²½ê³ : .env íŒŒì¼ì—ì„œ GOOGLE_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        print(\"âœ… VS Code/Codespaces í™˜ê²½: API í‚¤ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ì—†ì„ ê²½ìš° ìë™ ì„¤ì¹˜ ì•ˆë‚´ ë©”ì‹œì§€)\n",
    "try:\n",
    "    import langchain_google_genai\n",
    "    import pymupdf\n",
    "    import chromadb\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì•„ë˜ ëª…ë ¹ì–´ë¥¼ í„°ë¯¸ë„ì— ì‹¤í–‰í•˜ì„¸ìš”:\")\n",
    "    print(\"pip install langchain langchain-community langchain-google-genai pymupdf chromadb python-dotenv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ba497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def read_pdf_and_split_text_improved(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    [ê°œì„ ëœ í•¨ìˆ˜] PyMuPDFLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ PDFë¥¼ ì½ê³  ë” í° ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“‚ PDF ë¡œë“œ ì¤‘: {pdf_path} ...\")\n",
    "    \n",
    "    # 1. Loader ë³€ê²½: PyMuPDFLoaderê°€ í‘œ/ë ˆì´ì•„ì›ƒ ì²˜ë¦¬ì— í›¨ì”¬ ê°•ë ¥í•¨\n",
    "    loader = PyMuPDFLoader(pdf_path) \n",
    "    pages = loader.load()\n",
    "    \n",
    "    # 2. Splitter ì„¤ì •: chunk_sizeë¥¼ 1000ìœ¼ë¡œ ëŠ˜ë ¤ ë¬¸ë§¥(Context) ë³´ì¡´\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    \n",
    "    splits = text_splitter.split_documents(pages)\n",
    "    print(f\"âœ… ì´ {len(splits)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # (ë””ë²„ê¹…) ì²« ë²ˆì§¸ ì²­í¬ ë‚´ìš© í™•ì¸\n",
    "    if splits:\n",
    "        print(f\"\\n[ìƒ˜í”Œ ì²­í¬ ë‚´ìš©]:\\n{splits[10].page_content[:200]}...\\n\")\n",
    "        \n",
    "    return splits\n",
    "\n",
    "# ì‹¤í–‰ ì˜ˆì‹œ (íŒŒì¼ ê²½ë¡œëŠ” í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)\n",
    "pdf_path = r'C:\\Users\\ziclp\\OneDrive\\Desktop\\LLM Assignment\\1week\\PRACTICE1\\chap13\\2025í•™ë…„ë„ ì„œìš¸ í•™êµìš´ë™ë¶€ ìš´ì˜ ê³„íš(ë§¤ë‰´ì–¼)_ê³µë¬¸ ë°œì†¡ìš©.pdf'\n",
    "\n",
    "if os.path.exists(pdf_path):\n",
    "    docs = read_pdf_and_split_text_improved(pdf_path)\n",
    "else:\n",
    "    print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b546b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# 1. ì„ë² ë”© ëª¨ë¸ ì„¤ì •\n",
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "\n",
    "# 2. ë²¡í„° ì €ì¥ì†Œ ìƒì„± (ë©”ëª¨ë¦¬ ëª¨ë“œ)\n",
    "# ì˜êµ¬ ì €ì¥ì„ ì›í•˜ë©´ persist_directory=\"./chroma_db\" ì˜µì…˜ ì¶”ê°€\n",
    "if 'docs' in locals() and docs:\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embedding\n",
    "    )\n",
    "    print(\"âœ… ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "    # 3. ê²€ìƒ‰ê¸° ì„¤ì • (k=10ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ 38í˜ì´ì§€ ë‚´ìš©ì´ ë°€ë ¤ë‚˜ì§€ ì•Šë„ë¡ í•¨)\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "else:\n",
    "    print(\"âš ï¸ ë¬¸ì„œê°€ ë¡œë“œë˜ì§€ ì•Šì•„ ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ff8730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 1. LLM ì„¤ì •\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# 2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "rag_prompt_template = \"\"\"\n",
    "ë‹¹ì‹ ì€ í•™êµ ìš´ë™ë¶€ ìš´ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ì˜ [Context]ë¥¼ ë°”íƒ•ìœ¼ë¡œ [Question]ì— ëŒ€í•´ ìƒì„¸íˆ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n",
    "ë§Œì•½ [Context]ì— ì—†ëŠ” ë‚´ìš©ì´ë¼ë©´ \"ì œê³µëœ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"ë¼ê³  ë‹µí•˜ì„¸ìš”.\n",
    "\n",
    "[Context]:\n",
    "{context}\n",
    "\n",
    "[Question]:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = PromptTemplate.from_template(rag_prompt_template)\n",
    "\n",
    "# 3. ë¬¸ì„œ í¬ë§·íŒ… í•¨ìˆ˜\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# 4. RAG ì²´ì¸ êµ¬ì„±\n",
    "if 'retriever' in locals():\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | rag_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # 5. ì‹¤í–‰ ë° ê²°ê³¼ ì¶œë ¥\n",
    "    query = \"ìš´ë™ë¶€ ì „ì§€í›ˆë ¨ ê³„íšì— ëŒ€í•´ ì•Œë ¤ì¤˜\"\n",
    "    print(f\"â“ ì§ˆë¬¸: {query}\\n\")\n",
    "    response = rag_chain.invoke(query)\n",
    "    print(f\"ğŸ’¡ ë‹µë³€:\\n{response}\")\n",
    "else:\n",
    "    print(\"âš ï¸ ê²€ìƒ‰ê¸°ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc964dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "chat = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\", # ì›ë³¸ ëª¨ë¸ ìœ ì§€\n",
    "    api_key=GOOGLE_API_KEY,\n",
    "    temperature=0.3\n",
    ")\n",
    "chat.invoke('ìš´ë™ë¶€ ì „ì§€í›ˆë ¨ ê³„íš ì‘ì„±ë°©ë²• ì•Œë ¤ì¤˜')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
