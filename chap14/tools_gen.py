from tavily import TavilyClient
from langchain_core.tools import tool
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from datetime import datetime
import json
import os
absolute_path = os.path.abspath(__file__) #í˜„ì¬ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œ ë°˜í™˜
current_path = os.path.dirname(absolute_path) # í˜„ì¬ .py íŒŒì¼ì´ ìˆëŠ” í´ë” ê²½ë¡œ
from dotenv import load_dotenv
from langchain_community.document_loaders import WebBaseLoader

# --- .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ ---
load_dotenv()

# --- .env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ ---
google_api_key = os.getenv("GOOGLE_API_KEY")
# --- [ì¶”ê°€] Tavily API í‚¤ ë¡œë“œ ---
tavily_api_key = os.getenv("TAVILY_API_KEY")

# --- .ì„ë² ë”© ëª¨ë¸ ë° ì €ì¥ ê²½ë¡œ ì„¤ì •
embedding = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
persist_directory = './chroma_store_gemini_v1'

# Chroma ê°ì²´ ìƒì„±
vectorstore = Chroma(
    persist_directory=persist_directory,
    embedding_function=embedding
)


@tool
def web_search(query: str):
    """
    ì£¼ì–´ì§„ queryì— ëŒ€í•´ ì›¹ ê²€ìƒ‰ì„ í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•œë‹¤.

    Args:
        query (str): ê²€ìƒ‰ì–´

    returns:
        dict: ê²€ìƒ‰ ê²°ê³¼    
    """
    client = TavilyClient()
    
    content = client.search(
        query,
        search_depth="advanced",
        included_raw_content = True,
    )

    results = content["results"]

    for result in results:
        if result["raw_content"] is None:
            try:
                result["raw_content"] = load_web_page(result["url"])
            except Exception as e:
                print(f"Error loading page: {result['url']}")
                print(e)
                result["raw_content"] = result["content"]

    resources_json_path = f'{current_path}/data/resources_{datetime.now().strftime('%Y_%m%d_%H%M%S')}.json'
    with open(resources_json_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=4)
   
    return results, resources_json_path  # ê²€ìƒ‰ ê²°ê³¼ì™€ JSON íŒŒì¼ ê²½ë¡œ ë°˜í™˜

def web_page_to_document(web_page):
    # raw_contentì™€ content ì¤‘ ì •ë³´ê°€ ë§ì€ ê²ƒì„ page_contentë¡œ í•œë‹¤.
    if len(web_page['raw_content']) > len(web_page['content']):
        page_content = web_page['raw_content']
    else:
        page_content = web_page['content']
    # ë­ì²´ì¸ Documentë¡œ ë³€í™˜
    document = Document(
        page_content=page_content,
        metadata={
            'title': web_page['title'],
            'source': web_page['url']
        }
    )

    return document

def web_page_json_to_documents(json_file):
    with open(json_file, "r", encoding='utf-8') as f:
        resources = json.load(f)

    documents = []

    for web_page in resources:
        document = web_page_to_document(web_page)
        documents.append(document)

    return documents

def split_documents(documents, chunk_size=1000, chunk_overlap=100):
    """
    ë¬¸ì„œë¥¼ ì§€ì •ëœ í¬ê¸°(chunk_size)ì™€ ì¤‘ì²©(chunk_overlap)ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    """
    print('Splitting documents...')
    print(f"{len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ {chunk_size}ì í¬ê¸°ë¡œ ì¤‘ì²© {chunk_overlap}ìë¡œ ë¶„í• í•©ë‹ˆë‹¤.\n")

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, chunk_overlap=chunk_overlap
    )

    splits = text_splitter.split_documents(documents)

    print(f"ì´ {len(splits)}ê°œì˜ ë¬¸ì„œë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return splits

def documents_to_chroma(documents, chunk_size=1000, chunk_overlap=100):
    """
    ë¬¸ì„œë¥¼ í™•ì¸í•˜ì—¬ ì´ë¯¸ ë²¡í„° DBì— ì¡´ì¬í•˜ëŠ” ì†ŒìŠ¤(URL ë“±)ëŠ” ê±´ë„ˆë›°ê³ ,
    ìƒˆë¡œìš´ ë¬¸ì„œë§Œ ë²¡í„° DB(Chroma)ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    print("Documentsë¥¼ Chroma DBì— ì €ì¥ ì‹œë„ ì¤‘...")

    # 1. ì…ë ¥ëœ ë¬¸ì„œë“¤ì—ì„œ ì†ŒìŠ¤(URL) ëª©ë¡ ì¶”ì¶œ
    # metadataì— 'source' í‚¤ê°€ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ .get() ì‚¬ìš©
    input_urls = [doc.metadata.get('source') for doc in documents if doc.metadata.get('source')]
    
    # 2. ê¸°ì¡´ ë²¡í„° DBì— ì €ì¥ëœ ë°ì´í„° ì¡°íšŒ (LangChain ì¸í„°í˜ì´ìŠ¤ í™œìš©)
    # vectorstore.get()ì€ DBì˜ ëª¨ë“  ë©”íƒ€ë°ì´í„° ë“±ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    existing_data = vectorstore.get()
    existing_urls = set()

    if existing_data['metadatas']:
        for metadata in existing_data['metadatas']:
            # ë©”íƒ€ë°ì´í„°ê°€ ì¡´ì¬í•˜ê³  'source' í‚¤ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ì¶œ
            if metadata and 'source' in metadata:
                existing_urls.add(metadata['source'])
    
    print(f" - ê¸°ì¡´ DB ì €ì¥ ë¬¸ì„œ ìˆ˜: {len(existing_data['ids'])}ê°œ")
    print(f" - ê¸°ì¡´ DB ì†ŒìŠ¤(URL) ìˆ˜: {len(existing_urls)}ê°œ")

    # 3. ì¤‘ë³µë˜ì§€ ì•Šì€ ìƒˆë¡œìš´ URL ì‹ë³„ (ì§‘í•© ì—°ì‚° ì°¨ì§‘í•© í™œìš©)
    new_urls = set(input_urls) - existing_urls
    
    if not new_urls:
        print("âœ… ì¶”ê°€í•  ìƒˆë¡œìš´ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. (ëª¨ë“  ë¬¸ì„œê°€ ì´ë¯¸ DBì— ì¡´ì¬í•¨)")
        return

    print(f"âœ¨ ìƒˆë¡œ ì¶”ê°€í•  ì†ŒìŠ¤(URL) {len(new_urls)}ê°œë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")

    # 4. ìƒˆë¡œìš´ URLì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œë§Œ í•„í„°ë§
    new_documents = [doc for doc in documents if doc.metadata.get('source') in new_urls]

    if new_documents:
        # 5. ìƒˆë¡œìš´ ë¬¸ì„œ ë¶„í•  (Splitting)
        splits = split_documents(new_documents, chunk_size=chunk_size, chunk_overlap=chunk_overlap)

        # 6. ë²¡í„° DBì— ì¶”ê°€ (Embedding & Indexing)
        # add_documentsë¥¼ í˜¸ì¶œí•˜ë©´ ìë™ìœ¼ë¡œ ì„ë² ë”©ë˜ì–´ ì €ì¥ë©ë‹ˆë‹¤.
        if splits:
            vectorstore.add_documents(splits)
            print(f"ğŸš€ {len(splits)}ê°œì˜ ìƒˆë¡œìš´ ì²­í¬ê°€ ë²¡í„° DBì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("âš  ë¶„í• ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("ì²˜ë¦¬í•  ìƒˆë¡œìš´ ë¬¸ì„œ ê°ì²´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

def add_web_pages_json_to_chroma(json_file, chunk_size=1000, chunk_overlap=100):
    """
    JSON íŒŒì¼ì—ì„œ ì›¹ í˜ì´ì§€ ì •ë³´ë¥¼ ì½ì–´ì™€ Documentë¡œ ë³€í™˜ í›„,
    ì¤‘ë³µì„ ì²´í¬í•˜ì—¬ Chroma DBì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    # tools_gen.pyì— ì •ì˜ëœ í•¨ìˆ˜ë¼ê³  ê°€ì • (import í•„ìš” ì‹œ í™•ì¸)
    documents = web_page_json_to_documents(json_file)
    
    if not documents:
        print("âŒ JSON íŒŒì¼ì—ì„œ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    documents_to_chroma(
        documents, 
        chunk_size=chunk_size, 
        chunk_overlap=chunk_overlap
    )


def load_web_page(url: str):
    loader = WebBaseLoader(url, verify_ssl=False)
    content = loader.load()
    raw_content = content[0].page_content.strip()

    while '\n\n\n' in raw_content or '\t\t\t' in raw_content:
        raw_content = raw_content.replace('\n\n\n', '\n\n')
        raw_content = raw_content.replace('\t\t\t', '\t\t')
        
    return raw_content


@tool
def retrieve(query: str, top_k: int=5):
    """
    ì£¼ì–´ì§„ queryì— ëŒ€í•´ ë²¡í„° ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³ , ê²°ê³¼ë¥¼ ë°˜í™˜í•œë‹¤.
    """
    retriever = vectorstore.as_retriever(search_kwargs={"k": top_k})
    retrieved_docs = retriever.invoke(query)

    return retrieved_docs



if __name__ == "__main__":
    #results, resources_json_path = web_search.invoke("ëŒ€í•œë¯¼êµ­ í•µë¬´ì¥ ê°€ëŠ¥ì„±")
    #print(results)
    #documents = web_page_json_to_documents(f'{current_path}/data/resources_2025_1126_172343.json')  
    #splits = split_documents(documents)
    #print(splits)
    #add_web_pages_json_to_chroma(f'{current_path}/data/resources_2025_1126_202523.json')
    retrieved_docs = retrieve.invoke({"query": "ëŒ€í•œë¯¼êµ­ í•µë¬´ì¥ ê°€ëŠ¥ì„±"})
    print(retrieved_docs)
